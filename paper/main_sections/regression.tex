% tex file for regression
\par \indent A simple and straightforward way to model the voxel time courses 
for each subject is to perform linear regression. Initially, we just used the 
convolved predicted hemodyamic response (HR) and either all of the 
conditions together or each of conditions individually. After realizing that 
the HRs themselves didn't explain enough of the BOLD ratio, we attempted to 
add features in order to reduce or explain the noise we observed. Additional 
features that we examined included a linear drift and some of the time 
courses' Fourier series and principal components.

\par Linear regression assumes a linear relationship between a response 
vector $y$ and a design matrix of predictors $X$. Each element of $y$ 
represents a single observed response, and each row of $X$ represents a 
corresponding vector of predictor values. If one including the intercept as a 
term (as we have elected to do), the first column of the $X$ design matrix 
should be a vector of $1$s. The linear model can then be expressed as: 

\begin{equation}
y = X\beta + \epsilon
\end{equation}

\par It is further assumed that the errors $\epsilon_i$ for each observation 
$i$ are independent and identically distributed with $N(0, \sigma^2)$, and 
that the errors are independent of $X$. The vector of coefficients $\beta$ 
with length equal to the number of predictors in $X$ can be estimated with 
the closed-form solution:

\begin{equation}
\hat{\beta} =(X^T X)^{-1} X^T y
\end{equation}

\par Even when $(X^T X)$ is not invertible, $\hat{\beta}$ can be estimated 
using the pseudo-inverse of $(X^T X)$, represented as $(X^T X)^{-}$ to get a 
non-unique value for $\hat{\beta}$.

\par To consider the strength of the effects of these predictors, we will use 
t-tests of the corresponding estimated coefficients for each voxel and 
subject, as discussed under Section \ref{hypothesis_testing}. The validity of 
the model and of the ``p-values'' produced by performing these t-tests is d
ependent on whether or not the many assumptions of the linear model are 
actually met. In particular, we will discuss the assumption of normal errors 
by analyzing the residuals in Section \ref{normality}. 

\subsubsection{More about potential features:}
\par Other than the basic HR feature(s) and a column of $1$s (to account 
for an ``intercept'' term or non-zero average value), we experimented with 
additional predictors for our design matrix $X$. Among these were the 
first few principal components of the voxel $\times$ time matrix of voxel time 
courses and the first few functions of the Fourier series for the time courses. 
As noted above these, additional features helped account for the noise in the 
observed BOLD ratio fluctuation.

\vspace{2mm}
\noindent \textbf{Principal Components}
\vspace{2mm}
\par One approach for reducing the noise in the linear model is to include 
principal components of the voxel $\times$ time voxel time course matrix. 
Instead of using the entire matrix, it may be possible to just include the 
first few principal components as features that explain a great deal of the 
variance in the entire matrix. To get the principal components, we obtained 
the singular value decomposition (SVD) of the time $\times$ time covariance 
matrix. We tried this with and without first masking the voxels. To 
standardize the voxels, we subtracted the column means (mean across voxels) 
from the voxel by time matrix. There is also a very strong effect of mean 
over time in the data that dominates other effects, so we subtracted the 
row means (mean over time) as well.

As you can see in Figure \ref{fig:pca10}, which compares the variance 
explained by including up to ten components, with and without masking the 
voxels, masking explains more variance at each component. This trend was 
observed across all subjects. So, between the better performance and the 
logical rationality of using the masked data (we are not actually interested 
in the behvavior of voxels outside the brain), we decided to only work with 
the masked data's principal components. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=.5\linewidth]{../images/pcacumsumssub010.png}
 	\caption{Comparing proportion of variance explained by Subject 10's 
principal components, with and without masking the data.}
 	\label{fig:pca10}
\end{figure}

An important issue to consider is how many principal components to include in 
the design matrix. Figure \ref{fig:pcabox} compares the the amount of variance 
explained by including successively more principal components across subjects. 
A few observations should be noted. First, there is considerable variation 
between subjects in how much variance the early principal components capture. 
Second, by including only the first six components, it is possible for most 
subjects' voxel time course matrix to capture at least 40\% of the variance. 
Moving forward, we chose to include six principal components as additional 
features when considering models that reduce noise. This cutoff of six 
components or 40\% of the variance explained was somewhat arbitrary, with the 
idea being we wanted to only include a few components without sacrificing too 
much of the variance explained and that we wanted to use the same number of 
components for each subject. It will be seen later that the variation between 
subjects in how much variance is captured by the first six components has strong 
ramifications on the results. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=.5\linewidth]{../images/pcaBOX.png}
 	\caption{Boxplots comparing the amount of variance captured by principal 
components for each subject. The data was masked beforehand. All subjects are 
able to capture at least 40\% of the variance when using just six principal 
components.}
 	\label{fig:pcabox}
\end{figure}


\vspace{2mm}
\noindent \textbf{Fourier Series}
\vspace{2mm}
\par We included 6 features related to the first few functions of the Fourier series.
A full Fourier series is represented as the following:

\begin{equation}
f(x) = \frac{1}{2} \cdot a_0 + \sum_{n=1}^{\infty} a_n \cdot cos(n x) + \sum_{n=1}^{\infty} b_n \cdot  sin(n x)
\end{equation}

\noindent We wanted to represent low level sinesoidal fluctuations, which required a 
few periods over the full range of the time course $(0, \text{num of TR})$, as such 
we changed the Fourier series to:

\begin{equation}
f(x) = \frac{1}{2} \cdot a_0 + \sum_{n=1}^{\infty} a_n \cdot cos(\frac{n}{\text{num of TR}} x) + \sum_{n=1}^{\infty} b_n \cdot sin(\frac{n}{\text{num of TR}} x)
\end{equation}

\noindent We used $ \sum_{n=1}^{3} a_n \cdot cos(\frac{n}{\text{num of TR}} x) + 
\sum_{n=1}^{3} b_n \cdot sin(\frac{n}{\text{num of TR}} x)$ to be 6 features to try 
to get a low order sinusoidal fluxucations.

\subsection{Model Selection} \label{model_selection}

\par In order to select the best set of features for our $X$ matrix, and also to 
compare the use of a single condition feature vs. each of the three different 
types of conditions as three seperate features, we decided to utilize model 
comparison metric; specificially, AIC, BIC, and adjusted $R^2$ metrics. Using a small but  
expressive subset of the subjects ($002$,$003$, and $014$) we averaged the metrics across all voxels and people, a not-especially theoretically sound approach. We visualized values in Figures \ref{fig:AIC},\ref{fig:BIC}, and \ref{fig:adjr2}. 

\par From these plots we can observe that \textbf{(1)} seperating the conditions into
individual features did not provide much gain in these metrics and \textbf{(2)}
the inclusion of the 6 principal components comparably tends to create better models than the 
inclusion of the 6 Fourier series features. We initially interpreted 
this as a vote to include the first 6 principal components and not the Fourier 
features. Unfortunately, including the 6 principal components lead to 
overfitting and collinearity with the HRF features for some subjects. These problems tended to arise
when the proportion of variance explained by the 6 principal components was much greater that 40 \%.
Overall, we went with the Fourier features and observed similar t-
statistics for the HRF feature in the models with the 6 principal components vs
the 6 Fourier features when the variance explained was $\leq$ 40 \%.


\begin{figure}
\centering
	\begin{minipage}[b]{0.33\linewidth}
		\centering
		\includegraphics[width=.8\linewidth]{../images/aic_better}  

		\caption{AIC}
		\label{fig:AIC}

	\end{minipage}
	\quad
	\begin{minipage}[b]{0.33\linewidth}
		\centering
		\includegraphics[width=.8\linewidth]{../images/bic_better}  
		\caption{BIC}
		\label{fig:BIC}

	\end{minipage}
		
	\begin{minipage}[b]{0.33\linewidth}
		\centering
		\includegraphics[width=.8\linewidth]{../images/adjr2_better}  
		\caption{Adjusted $R^2$}
		\label{fig:adjr2}

	\end{minipage}

\end{figure}

	

