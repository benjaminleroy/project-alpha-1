% tex file for clustering 

\subsection{Benjamini-Hochberg Correction}

\par \indent When conducting multiple comparisons, it is important to have an 
idea of the quantity of Type I errors that may be prevalent in the analysis. 
In our analysis of voxel data, we decided that limiting/controlling the number 
of Type I errors is important to the process. The processes of limiting the 
number of Type I errors are called FDR-controlling procedures. In the grand 
scheme of things, FDR-controlling procedures give greater statistical power 
with the cost of more Type I errors that can fall through. 

\par Once we implemented the hypothesis function that would return t-test 
values and ``p-values'', we implemented the Benjamini-Hochberg procedure to 
control the proportion of rejected null hypotheses in the data. The Benjamini-
Hochberg procedure works by multiplying each of the ``p-values'' to a ratio of 
the number of tests and the chosen false discovery rate -- from these adjusted 
``p-values'', only the values that are less than the chosen false discovery rate 
will be chosen to be returned. This way, we are able to adjust the proportion of 
null hypotheses that will be rejected and the proportion that will return the 
desired proportion of significant tests. This will reduce the number of false 
positives returned in the data and extend greater statistical power in later 
analysis performed on the voxel dataset.

\subsection{Hierarchical Clustering}

\par Now that we have the across-subject average t-statistics for every voxel 
in the brain, we are left with a 3-d array of t-statistics that contain both 
negative and positive values. Instead of manually observing patterns in these 
images, we instead implemented a clustering algorithm to split the entire 3-d 
images into clusters based on the voxels' relative location to each other as 
well as the values of their t-statistics.

\par In order to find a proper clustering algorithm, we decided to treat this
problem like a grayscale image segmentation problem and implemented a
agglomerative hierarchical cluster using Ward's method. Agglomerative means
that the clusters are built bottom up which each observation starting as its
own cluster and pairs being moved up the hierarchy. Ward's method creates
clusters based on a minimum variance criterion that miniizes the total
within-cluster variances. An example of this implimentation for a 2d image is
seen here: 
\url{http://scikit-learn.org/stable/auto_examples/cluster/plot_lena_ward_segment
 ation.html}.

In our implimentation, we defined a structure to our data using a connectivity
graph in order to ensure that each cluster is spatially constrained. Also,
since our scenario uses a 3d image, the connectivity graph will also have to
take into account this extra dimension.


\par Ultimately, the goal of clustering is to have a better understanding of 
which parts of the brain are related to the signal based on the t-statistics 
from performing hypothesis tests on the coefficients from linear regression.
Once we obtain our clusters, we will both measure the within-cluster mean of
t-statistics as well measure the centroids of the clusters. By doing this, we hope
to see the parts of the brain that have the strongest relationship with the
signal and compare them to the results of the origin research paper.

\subsection{t-Statisics Grouping}

\par We use the t-statistics that we found earlier for another type of cutoff
analysis. The t-statistics measure the size of the difference relative to the 
variation in the data. Although the t-statistics go hand in hand with the 
Benjamini-Hochberg analysis on the ``p-values'', we ultimately decided that an 
additional process of selecting t-statistics based on a threshold was 
necessary because the ``p-values'' assume a normal distribution while the 
t-statistics do not assume this from the data.

\par Since the magnitude from zero represents how significant the test is for
t-statistics, we collected the absolute value t-statistics that were above a 
certain threshold. This was the opposite of the procedure for finding the 
significant tests in the Benjamini-Hochberg process. The threshold, also called
the cutoff in the scripts that were written when developing this process, is 
calculated in a very similar fashion to the Benjamini-Hochberg false discovery 
rate. This is because the t-values and the ``p-values'' are strongly linked,
because lower ``p-values'' and higher t-values are associated with significant 
findings. 

\par Implementing the t-value grouping function was almost trivial because it 
is the flip side of the Benjamini-Hochberg function that was implemented 
previously.

\subsection{Beta Grouping}

\par Using the same function that we created to group a certain subset of the 
t-statistics, we also looked over the beta-values as a part of our analysis. 
Much of what we know in statistics tells us that observing beta-values on top 
of looking at t-statistics would not tell us much since the two variables are 
not explicitly connected; however, the main reason why we decided it was of 
utmost importance to look at both the t-statistics and the beta values is that 
there might be a relationship between the trends in each respective variables'
output to the cutoff point. 