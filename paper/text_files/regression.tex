% tex file for regression
\par \indent A simple and straightforward way to model the voxel time courses 
is to perform simple and multiple linear regression. As a first attempt, we 
implemented and performed simple regression on a single subject's 4-
dimensional array of voxels against the convolved time course, such that 
every voxel had an intercept and a coefficient corresponding to the convolved 
time course. However, examining the effects of the BART experiment conditions 
on voxel blood flow is also of interest. Thus, we turned to a more 
sophisticated multiple linear regression model that includes the conditions 
as dummy variable predictors. We also considered multiple regression including 
linear drift and Fourier transform terms along with event conditions as dummy 
variables. In each scenario, we created a design matrix $X$ with the number of 
rows equal to the number of observed times and the number of rows is equal to 
the number of predictors. 

Our specified model is then $Y = X\beta + \epsilon$, where $X$ is the fixed 
aforementioned design matrix, $Y$ is the 4-dimensional array of the subject's 
observed voxel time courses transformed into 2-dimensional space, and $\epsilon$ 
is random noise. $\epsilon$ is assumed to be independent and identically 
distributed with a normal distribution $N(0, \sigma^2)$, and it is also assumed 
to be independent of $X$. We will need to check the validity of these 
assumptions, as discussed in the \textit{"Normality Assumptions"} section. 
$\beta$ is an unknown parameter that must be estimated using matrix algebra: 
$\hat{\beta} = (X X^T)^{-1} X^T Y$. To do this, we essentially flattened out the 
first three dimensions (which indicate spatial positions) into a single dimension, 
while keeping the fourth dimension (time) the same. The resulting $\hat{\beta}$ 
was transformed back into a 3-dimensional array to maintain the spatial 
relationships of the voxels. 

\par Another possible way to model the voxels is to use the first two principal 
components of the voxel by time covariance matrix for each subject as the $Y$ 
instead of the raw voxel time courses. This reduces dimensionality while still 
capturing much of the variance of the raw data. However, the choice to use two 
components is does not have an especially strong justification, as the proportion 
of variance explained by the components does not have a clear-cut "elbow" or 
plateau. 

\par To consider the strength of the effects of these predictors, we looked 
at t-tests of the corresponding estimated coefficients for each voxel, as 
discussed under \textit{"Hypothesis Testing"}. The validity of these t-tests and 
their corresponding "p-values" is largely dependent on how good our assumptions 
of linearity and normally-distributed errors are. Neither of these models, based 
on our current convolution methods, was very fruitful. 

